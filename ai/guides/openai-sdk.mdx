---
title: "Guide · OpenAI SDK"
description: "Practical patterns for Vercel AI SDK + OpenAI inside the AI Lab."
---

# Guide · OpenAI SDK

## 1. Stack
- **Runtime**: Next.js App Router (Route Handlers) or Edge Functions.
- **SDK**: `openai` official client + `ai` (Vercel AI SDK) for streaming.
- **Language**: TypeScript with strict types.

## 2. Boilerplate
```ts
import OpenAI from "openai";
import { StreamingTextResponse, experimental_StreamData } from "ai";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export async function POST(req: Request) {
  const { prompt } = await req.json();
  const response = await openai.responses.create({
    model: "gpt-4o-mini",
    input: prompt,
    temperature: 0.2,
  });
  return new Response(response.output[0].content[0].text);
}
```

## 3. Patterns
- **Function calling**: define tool schema with Zod; handle `response.output[0].content` accordingly.
- **Vision**: send base64 images or remote URLs for document QA.
- **Batching**: use the Batch API for large eval jobs; monitor status webhooks.

## 4. Cost Controls
- Use `gpt-4o-mini` for drafts, `gpt-4o` for final outputs.
- Log tokens per request; feed into cost observability dashboard.
- Cache responses (KV store) for deterministic prompts (e.g., onboarding instructions).

## 5. Safety
- Apply moderation endpoint before returning user-facing output.
- Combine with Guardrails (structured outputs + policy checks).

## 6. Testing
- Build fixtures using the Responses API `seed` parameter for repeatability.
- Run contract tests in CI hitting OpenAI mock server (e.g., `msw`).

Fork my `sentience-sdk-starter` (linked in repo) to get going fast.

<Note>
Next up: [Aussie →](/australia/basics)
</Note>
